{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project 2 Code**\n",
    "=================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Places API Clustering*** (No Visualisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44 valid stop entries\n",
      "Found 6 significant location clusters\n",
      "   cluster                     name    types         vicinity\n",
      "0        0     16581-16473 Hayes Ln  [route]       Woodbridge\n",
      "1        1  113-105 Observatory Ave  [route]  Charlottesville\n",
      "2        2         130 Chemistry Dr  [route]       University\n",
      "3        3     284-294 McCormick Rd  [route]  Charlottesville\n",
      "4        4    Jefferson Park Avenue  [route]  Charlottesville\n",
      "5        5          Michigan Avenue  [route]          Chicago\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "from haversine import haversine\n",
    "from sklearn.cluster import DBSCAN\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Load stops from location-history.json ---\n",
    "def load_stops(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    records = []\n",
    "    for entry in data:\n",
    "        if \"visit\" in entry:\n",
    "            loc = entry[\"visit\"][\"topCandidate\"][\"placeLocation\"]\n",
    "            if loc.startswith(\"geo:\"):\n",
    "                lat, lng = map(float, loc[4:].split(\",\"))\n",
    "                try:\n",
    "                    start = parser.parse(entry[\"startTime\"])\n",
    "                    end = parser.parse(entry[\"endTime\"])\n",
    "                    duration = (end - start).total_seconds() / 60\n",
    "                    if duration >= 10:\n",
    "                        records.append({\"lat\": lat, \"lng\": lng, \"duration_min\": duration})\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping bad entry: {e}\")\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Cluster using DBSCAN + haversine\n",
    "def cluster_locations(df, eps_meters=100):\n",
    "    coords = df[['lat', 'lng']].values\n",
    "    radians = np.radians(coords)  # Convert to radians for haversine\n",
    "\n",
    "    eps_km = eps_meters / 1000.0\n",
    "    kms_per_radian = 6371.0088\n",
    "    eps = eps_km / kms_per_radian\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=2, metric='haversine')\n",
    "    df['cluster'] = db.fit_predict(radians)\n",
    "    return df[df['cluster'] != -1]  # drop noise\n",
    "\n",
    "# Call Google Places API\n",
    "def reverse_geocode(lat, lng):\n",
    "    try:\n",
    "        url = f\"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "        params = {\n",
    "            'location': f\"{lat},{lng}\",\n",
    "            'radius': 50,\n",
    "            'key': GOOGLE_API_KEY\n",
    "        }\n",
    "        response = requests.get(url, params=params).json()\n",
    "        if response['status'] == 'OK' and response['results']:\n",
    "            top = response['results'][0]\n",
    "            return {\n",
    "                \"name\": top.get(\"name\", \"\"),\n",
    "                \"types\": top.get(\"types\", []),\n",
    "                \"vicinity\": top.get(\"vicinity\", \"\")\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"API error for ({lat}, {lng}): {e}\")\n",
    "    return {}\n",
    "\n",
    "def label_clusters(df):\n",
    "    centroids = df.groupby('cluster')[['lat', 'lng']].mean().reset_index()\n",
    "    labels = []\n",
    "\n",
    "    for _, row in centroids.iterrows():\n",
    "        lat, lng = row['lat'], row['lng']\n",
    "        place = reverse_geocode(lat, lng)\n",
    "        label = {\n",
    "            \"cluster\": int(row['cluster']),\n",
    "            \"lat\": lat,\n",
    "            \"lng\": lng,\n",
    "            \"name\": place.get(\"name\", \"\"),\n",
    "            \"types\": place.get(\"types\", []),\n",
    "            \"vicinity\": place.get(\"vicinity\", \"\")\n",
    "        }\n",
    "        labels.append(label)\n",
    "        time.sleep(1)  # avoid rate limiting\n",
    "\n",
    "    return pd.DataFrame(labels)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stops_df = load_stops(\"location-history.json\")\n",
    "    print(f\"Loaded {len(stops_df)} valid stop entries\")\n",
    "\n",
    "    clustered_df = cluster_locations(stops_df)\n",
    "    print(f\"Found {clustered_df['cluster'].nunique()} significant location clusters\")\n",
    "\n",
    "    labeled_df = label_clusters(clustered_df)\n",
    "    labeled_df.to_csv(\"significant_locations.csv\", index=False)\n",
    "\n",
    "    print(labeled_df[['cluster', 'name', 'types', 'vicinity']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Places API Clustering*** (Visualisation, Altered Cluster Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Visualizations and labeled clusters saved, including time-of-day and route maps.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap, TimestampedGeoJson\n",
    "from dateutil import parser\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# --- Load API Key from .env ---\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# --- Load and parse location-history.json ---\n",
    "def parse_location_history(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    visits = []\n",
    "    for entry in raw_data:\n",
    "        if \"visit\" in entry and \"topCandidate\" in entry[\"visit\"]:\n",
    "            loc = entry[\"visit\"][\"topCandidate\"].get(\"placeLocation\", \"\")\n",
    "            if loc.startswith(\"geo:\"):\n",
    "                lat, lng = map(float, loc[4:].split(\",\"))\n",
    "                try:\n",
    "                    start = parser.parse(entry[\"startTime\"])\n",
    "                    end = parser.parse(entry[\"endTime\"])\n",
    "                    duration = (end - start).total_seconds() / 60\n",
    "                    if duration >= 5:\n",
    "                        visits.append({\n",
    "                            \"start_time\": start,\n",
    "                            \"end_time\": end,\n",
    "                            \"duration_min\": duration,\n",
    "                            \"lat\": lat,\n",
    "                            \"lng\": lng,\n",
    "                            \"date\": start.date(),\n",
    "                            \"hour\": start.hour\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "    return pd.DataFrame(visits)\n",
    "\n",
    "# --- Cluster locations to reduce API calls ---\n",
    "def cluster_locations(df, eps_meters=15):\n",
    "    coords = df[[\"lat\", \"lng\"]].values\n",
    "    radians = np.radians(coords)  # Convert to radians for haversine\n",
    "\n",
    "    eps_km = eps_meters / 1000.0\n",
    "    kms_per_radian = 6371.0088\n",
    "    eps = eps_km / kms_per_radian\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=2, metric='haversine')\n",
    "    df['cluster'] = db.fit_predict(radians)\n",
    "    return df[df['cluster'] != -1]  # drop noise\n",
    "\n",
    "# --- Reverse geocode using Google Places API ---\n",
    "def reverse_geocode(lat, lng):\n",
    "    try:\n",
    "        url = f\"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "        params = {\n",
    "            'location': f\"{lat},{lng}\",\n",
    "            'radius': 35,\n",
    "            'key': GOOGLE_API_KEY\n",
    "        }\n",
    "        response = requests.get(url, params=params).json()\n",
    "        if response['status'] == 'OK' and response['results']:\n",
    "            top = response['results'][0]\n",
    "            return {\n",
    "                \"name\": top.get(\"name\", \"\"),\n",
    "                \"types\": top.get(\"types\", []),\n",
    "                \"vicinity\": top.get(\"vicinity\", \"\")\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"API error for ({lat}, {lng}): {e}\")\n",
    "    return {}\n",
    "\n",
    "# --- Label clustered locations ---\n",
    "def label_clusters(df):\n",
    "    centroids = df.groupby('cluster')[['lat', 'lng']].mean().reset_index()\n",
    "    labels = []\n",
    "\n",
    "    for _, row in centroids.iterrows():\n",
    "        lat, lng = row['lat'], row['lng']\n",
    "        place = reverse_geocode(lat, lng)\n",
    "        label = {\n",
    "            \"cluster\": int(row['cluster']),\n",
    "            \"centroid_lat\": lat,\n",
    "            \"centroid_lng\": lng,\n",
    "            \"name\": place.get(\"name\", \"\"),\n",
    "            \"types\": place.get(\"types\", []),\n",
    "            \"vicinity\": place.get(\"vicinity\", \"\")\n",
    "        }\n",
    "        labels.append(label)\n",
    "        time.sleep(1)  # avoid rate limiting\n",
    "\n",
    "    return pd.DataFrame(labels)\n",
    "\n",
    "# --- Google Directions API route matching ---\n",
    "def get_directions_route(start_lat, start_lng, end_lat, end_lng):\n",
    "    try:\n",
    "        url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
    "        params = {\n",
    "            \"origin\": f\"{start_lat},{start_lng}\",\n",
    "            \"destination\": f\"{end_lat},{end_lng}\",\n",
    "            \"mode\": \"walking\",\n",
    "            \"key\": GOOGLE_API_KEY\n",
    "        }\n",
    "        response = requests.get(url, params=params).json()\n",
    "        if response['status'] == 'OK':\n",
    "            steps = response['routes'][0]['legs'][0]['steps']\n",
    "            return [(step['start_location']['lat'], step['start_location']['lng']) for step in steps] + \\\n",
    "                   [(steps[-1]['end_location']['lat'], steps[-1]['end_location']['lng'])]\n",
    "    except:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "# --- Generate visualizations ---\n",
    "def generate_visualizations(df, labels_df):\n",
    "    df = df.merge(labels_df[['cluster', 'name']], on='cluster', how='left')\n",
    "\n",
    "    # Bar chart: Average time spent per cluster\n",
    "    avg_time = df.groupby('name')['duration_min'].mean().sort_values(ascending=False).reset_index()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=\"duration_min\", y=\"name\", data=avg_time, palette=\"Blues_d\")\n",
    "    plt.xlabel(\"Average Time Spent (minutes)\")\n",
    "    plt.ylabel(\"Location\")\n",
    "    plt.title(\"Average Time Spent per Significant Location\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"avg_time_per_cluster.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Route Map with cluster-based frequency indicator\n",
    "    m = folium.Map(location=[df[\"lat\"].mean(), df[\"lng\"].mean()], zoom_start=13)\n",
    "    cluster_counts = df.groupby('cluster').size().reset_index(name='count')\n",
    "    cluster_counts = cluster_counts.merge(labels_df, on='cluster')\n",
    "\n",
    "    for _, row in cluster_counts.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=(row[\"centroid_lat\"], row[\"centroid_lng\"]),\n",
    "            radius=5 + row[\"count\"] * 0.5,\n",
    "            popup=f\"{row['name']}<br>Visits: {row['count']}\",\n",
    "            color=\"red\",\n",
    "            fill=True,\n",
    "            fill_color=\"red\"\n",
    "        ).add_to(m)\n",
    "\n",
    "    sorted_df = df.sort_values(by=\"start_time\")\n",
    "    coords = sorted_df[[\"lat\", \"lng\"]].values.tolist()\n",
    "    for i in range(len(coords) - 1):\n",
    "        path = get_directions_route(*coords[i], *coords[i + 1])\n",
    "        if path:\n",
    "            folium.PolyLine(path, color=\"blue\", weight=3).add_to(m)\n",
    "\n",
    "    m.save(\"routes_with_frequencies.html\")\n",
    "\n",
    "    # Time-of-day activity heatmap per cluster\n",
    "    time_cluster = df.groupby([\"name\", \"hour\"])[\"duration_min\"].sum().unstack(fill_value=0)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(time_cluster, cmap=\"YlOrRd\", linewidths=.5, linecolor='gray', cbar_kws={'label': 'Minutes'})\n",
    "    plt.title(\"Time-of-Day Activity per Significant Location\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Location\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"time_of_day_by_location.png\")\n",
    "    plt.close()\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    df = parse_location_history(\"location-history.json\")\n",
    "    clustered_df = cluster_locations(df)\n",
    "    labeled_clusters = label_clusters(clustered_df)\n",
    "    if labeled_clusters is not None and not labeled_clusters.empty:\n",
    "        labeled_clusters.to_csv(\"labeled_clusters.csv\", index=False)\n",
    "        generate_visualizations(clustered_df, labeled_clusters)\n",
    "        print(\"Visualizations and labeled clusters saved, including time-of-day and route maps.\")\n",
    "    else:\n",
    "        print(\"No labeled clusters to save or visualize.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
